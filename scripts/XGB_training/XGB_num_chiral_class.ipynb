{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.load('../qm9_filtered.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_0 = []\n",
    "df_y_0 = []\n",
    "df_X_1 = []\n",
    "df_y_1 = []\n",
    "df_X_2 = []\n",
    "df_y_2 = []\n",
    "df_X_3 = []\n",
    "df_y_3 = []\n",
    "df_X_4 = []\n",
    "df_y_4 = []\n",
    "\n",
    "for line in df:\n",
    "    num = len(line['chiral_centers'])\n",
    "    \n",
    "    if num > 4:\n",
    "        continue\n",
    "    elif num == 0:\n",
    "        df_X_0.append(line['xyz'].flatten())\n",
    "        df_y_0.append(0)\n",
    "    elif num == 1:\n",
    "        df_X_1.append(line['xyz'].flatten())\n",
    "        df_y_1.append(1)\n",
    "    elif num == 2:\n",
    "        df_X_2.append(line['xyz'].flatten())\n",
    "        df_y_2.append(2)\n",
    "    elif num == 3:\n",
    "        df_X_3.append(line['xyz'].flatten())\n",
    "        df_y_3.append(3)\n",
    "    elif num == 4:\n",
    "        df_X_4.append(line['xyz'].flatten())\n",
    "        df_y_4.append(4)\n",
    "    \n",
    "df_X_0 = np.array(df_X_0)\n",
    "df_y_0 = np.array(df_y_0)\n",
    "df_X_1 = np.array(df_X_1)\n",
    "df_y_1 = np.array(df_y_1)\n",
    "df_X_2 = np.array(df_X_2)\n",
    "df_y_2 = np.array(df_y_2)\n",
    "df_X_3 = np.array(df_X_3)\n",
    "df_y_3 = np.array(df_y_3)\n",
    "df_X_4 = np.array(df_X_4)\n",
    "df_y_4 = np.array(df_y_4)\n",
    "    \n",
    "idx_0 = np.random.choice(np.arange(len(df_y_0)), 10000, replace=False)\n",
    "df_X_0 = df_X_0[idx_0]\n",
    "df_y_0 = df_y_0[idx_0]\n",
    "\n",
    "idx_1 = np.random.choice(np.arange(len(df_y_1)), 10000, replace=False)\n",
    "df_X_1 = df_X_1[idx_1]\n",
    "df_y_1 = df_y_1[idx_1]\n",
    "\n",
    "idx_2 = np.random.choice(np.arange(len(df_y_2)), 10000, replace=False)\n",
    "df_X_2 = df_X_2[idx_2]\n",
    "df_y_2 = df_y_2[idx_2]\n",
    "\n",
    "idx_3 = np.random.choice(np.arange(len(df_y_3)), 10000, replace=False)\n",
    "df_X_3 = df_X_3[idx_3]\n",
    "df_y_3 = df_y_3[idx_3]\n",
    "\n",
    "idx_4 = np.random.choice(np.arange(len(df_y_4)), 10000, replace=False)\n",
    "df_X_4 = df_X_4[idx_4]\n",
    "df_y_4 = df_y_4[idx_4]\n",
    "\n",
    "\n",
    "df_X = np.concatenate((df_X_0, df_X_1, df_X_2, df_X_3, df_X_4), axis=0)\n",
    "df_y = np.concatenate((df_y_0, df_y_1, df_y_2, df_y_3, df_y_4), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 50000\n"
     ]
    }
   ],
   "source": [
    "print(len(df_X), len(df_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6a97498577dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# number of trees in random forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "# number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "min_samples_leaf = [int(x) for x in np.linspace(start = 2, stop = 10, num = 5)]\n",
    "\n",
    "# number of features at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# max depth\n",
    "max_depth = [int(x) for x in np.linspace(100, 500, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# create random grid\n",
    "random_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_features': max_features,\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_leaf': min_samples_leaf\n",
    " }\n",
    "\n",
    "# Random search of parameters\n",
    "rfc_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, n_iter = 20, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the model\n",
    "rfc_random.fit(X_train, y_train)\n",
    "\n",
    "# print results\n",
    "print(rfc_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yzhou76/.local/lib/python3.9/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:41:19] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Train acc:  1.0\n",
      "test acc:  0.7627\n",
      "f1:  0.7621553036095737\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier(max_depth=20)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "\n",
    "print(\"Train acc: \", accuracy_score(y_train, y_train_pred))\n",
    "print(\"test acc: \", accuracy_score(y_test, y_pred))\n",
    "print(\"f1: \", f1_score(y_test, y_pred, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1731,  183,   70,   36,    6],\n",
       "       [ 249, 1397,  220,   73,   15],\n",
       "       [  64,  293, 1293,  236,   82],\n",
       "       [  15,   77,  225, 1503,  220],\n",
       "       [   8,   20,   79,  202, 1703]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (3.9.5)",
   "language": "python",
   "name": "python3-3.9.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
